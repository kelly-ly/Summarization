{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(sentences,stop_words):\n",
    "    corpus = []\n",
    "    for sent in sentences:\n",
    "        sent_token = nltk.word_tokenize(sent)\n",
    "        sent_list = [word for word in sent_token if word not in stop_words]\n",
    "        sent_str = ' '.join(sent_list)\n",
    "        corpus.append(sent_str)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "    return tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms to Evaluate the Importance of Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_frequency(tfidf):\n",
    "    weight = {}\n",
    "    for i in range(len(tfidf)):\n",
    "        weight[i] = np.sum(tfidf[i])\n",
    "\n",
    "    # Normalization\n",
    "    max_weight = max(weight.values())\n",
    "    min_weight = min(weight.values())\n",
    "    for key in weight.keys():\n",
    "        x = weight[key]\n",
    "        weight[key] = (x-min_weight)/(max_weight-min_weight)\n",
    "\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Position in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_position(sentences):\n",
    "    weight = {}\n",
    "    n = len(sentences)\n",
    "    for i in range(n):\n",
    "        weight[i] = (n - i)/n\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The similarity of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(s1, s2):\n",
    "    return np.sum(s1*s2)/(1e-6+(np.sqrt(np.sum(s1*s1))*np.sqrt(np.sum(s2*s2))))\n",
    "\n",
    "def weight_similarity(tfidf):\n",
    "    weight = collections.defaultdict(lambda :0)\n",
    "    for i in range(len(tfidf)):\n",
    "        score_i = 0\n",
    "        for j in range(len(tfidf)):\n",
    "            score_i += get_similarity(tfidf[i], tfidf[j])\n",
    "        weight[i] = score_i\n",
    "\n",
    "    # Normalization\n",
    "    max_score = max(weight.values())\n",
    "    min_score = min(weight.values())\n",
    "    for k in weight.keys():\n",
    "        x = weight[k]\n",
    "        weight[k] = (x-min_score)/(max_score-min_score)\n",
    "\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the composite score, then sort it in descending order (default: setting all of the weights as even)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_score(frequency, position, similarity, feature_weight = [1,1,1]):\n",
    "    weight = collections.defaultdict(lambda :0)\n",
    "    for k in frequency.keys():\n",
    "        weight[k] = feature_weight[0]*frequency[k]+feature_weight[1]*position[k]+feature_weight[2]*similarity[k]\n",
    "\n",
    "    return sorted(weight.items(), key = lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization(text, stop_word, num_sentence, comp_ratio = 0.3):\n",
    "    sentences = re.sub(r'\\n',' ',text) \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    tfidf = get_tfidf(sentences, stop_word)\n",
    "    frequency = weight_frequency(tfidf)\n",
    "    position = weight_position(sentences)\n",
    "    similarity = weight_similarity(tfidf)\n",
    "    \n",
    "    score = final_score(frequency, position, similarity, feature_weight = [1,1,1])\n",
    "    \n",
    "    if num_sentence is None:\n",
    "        num_sentence = int(len(sentences)*comp_ratio)\n",
    "    sent_id = sorted([sent[0] for sent in score[:num_sentence]])\n",
    "    \n",
    "    summary = []\n",
    "    for id in sent_id:\n",
    "        summary.append(sentences[id])\n",
    "\n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write example  000000_result.txt .\n",
      "Write example  000500_result.txt .\n",
      "Write example  001000_result.txt .\n",
      "Write example  001500_result.txt .\n",
      "Write example  002000_result.txt .\n",
      "Write example  002500_result.txt .\n",
      "Write example  003000_result.txt .\n",
      "Write example  003500_result.txt .\n",
      "Write example  004000_result.txt .\n",
      "Write example  004500_result.txt .\n",
      "Write example  005000_result.txt .\n",
      "Write example  005500_result.txt .\n",
      "Write example  006000_result.txt .\n",
      "Write example  006500_result.txt .\n",
      "Write example  007000_result.txt .\n",
      "Write example  007500_result.txt .\n",
      "Write example  008000_result.txt .\n",
      "Write example  008500_result.txt .\n",
      "Write example  009000_result.txt .\n",
      "Write example  009500_result.txt .\n",
      "Write example  010000_result.txt .\n",
      "Write example  010500_result.txt .\n",
      "Write example  011000_result.txt .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "article_dir = \"./my_evalue/articles\"\n",
    "result_dir = \"./my_evalue/basic\"\n",
    "\n",
    "stop_word = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for id in range(11490):\n",
    "    id = str(id)\n",
    "    article_name = str(id).zfill(6) + \"_article.txt\"\n",
    "    result_name = str(id).zfill(6) + \"_result.txt\"\n",
    "    article_path = os.path.join(article_dir, article_name)\n",
    "    result_path = os.path.join(result_dir, result_name)\n",
    "    \n",
    "    with open(article_path, \"r\") as f:\n",
    "        article_doc = f.read().splitlines()\n",
    "        current_article = \" \".join(article_doc)\n",
    "    \n",
    "    result = get_summarization(current_article, stop_word, num_sentence = 3)\n",
    "    \n",
    "    with open(result_path,\"w\") as f:\n",
    "        f.write(result)\n",
    "        if int(id)%500==0:\n",
    "            print(\"Write example \",result_name, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result by pyrouge:\n",
    "\n",
    "---------------------------------------------  \n",
    "1 ROUGE-1 Average_R: 0.31473 (95%-conf.int. 0.31261 - 0.31674)  \n",
    "1 ROUGE-1 Average_P: 0.53473 (95%-conf.int. 0.53215 - 0.53747)  \n",
    "1 ROUGE-1 Average_F: 0.38398 (95%-conf.int. 0.38207 - 0.38594)  \n",
    "\n",
    "---------------------------------------------  \n",
    "1 ROUGE-2 Average_R: 0.13426 (95%-conf.int. 0.13267 - 0.13581)  \n",
    "1 ROUGE-2 Average_P: 0.22805 (95%-conf.int. 0.22538 - 0.23062)  \n",
    "1 ROUGE-2 Average_F: 0.16363 (95%-conf.int. 0.16173 - 0.16540)  \n",
    "\n",
    "---------------------------------------------  \n",
    "1 ROUGE-3 Average_R: 0.07596 (95%-conf.int. 0.07452 - 0.07735)  \n",
    "1 ROUGE-3 Average_P: 0.12769 (95%-conf.int. 0.12541 - 0.13003)  \n",
    "1 ROUGE-3 Average_F: 0.09206 (95%-conf.int. 0.09045 - 0.09366)  \n",
    "\n",
    "---------------------------------------------  \n",
    "1 ROUGE-4 Average_R: 0.04972 (95%-conf.int. 0.04841 - 0.05096)  \n",
    "1 ROUGE-4 Average_P: 0.08276 (95%-conf.int. 0.08070 - 0.08482)  \n",
    "1 ROUGE-4 Average_F: 0.05991 (95%-conf.int. 0.05840 - 0.06139)  \n",
    "\n",
    "---------------------------------------------  \n",
    "1 ROUGE-L Average_R: 0.24827 (95%-conf.int. 0.24659 - 0.25000)  \n",
    "1 ROUGE-L Average_P: 0.42276 (95%-conf.int. 0.42031 - 0.42537)  \n",
    "1 ROUGE-L Average_F: 0.30315 (95%-conf.int. 0.30142 - 0.30493)  \n",
    "\n",
    "---------------------------------------------  \n",
    "1 ROUGE-W-1.2 Average_R: 0.07426 (95%-conf.int. 0.07370 - 0.07482)  \n",
    "1 ROUGE-W-1.2 Average_P: 0.30838 (95%-conf.int. 0.30645 - 0.31043)  \n",
    "1 ROUGE-W-1.2 Average_F: 0.11689 (95%-conf.int. 0.11610 - 0.11771)  \n",
    "\n",
    "---------------------------------------------  \n",
    "1 ROUGE-S* Average_R: 0.09381 (95%-conf.int. 0.09251 - 0.09503)  \n",
    "1 ROUGE-S* Average_P: 0.25627 (95%-conf.int. 0.25362 - 0.25901)  \n",
    "1 ROUGE-S* Average_F: 0.12408 (95%-conf.int. 0.12276 - 0.12535)  \n",
    "\n",
    "---------------------------------------------  \n",
    "1 ROUGE-SU* Average_R: 0.09889 (95%-conf.int. 0.09757 - 0.10012)  \n",
    "1 ROUGE-SU* Average_P: 0.26783 (95%-conf.int. 0.26517 - 0.27058)  \n",
    "1 ROUGE-SU* Average_F: 0.13084 (95%-conf.int. 0.12948 - 0.13213)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
